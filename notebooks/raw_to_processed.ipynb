{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a305aa55",
   "metadata": {},
   "source": [
    "# Raw to Processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77c457",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import matplotlib.dates as md\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42fbcc",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(ax, metric, data):\n",
    "    '''\n",
    "    Plots a metric on an axis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib Axes\n",
    "        axis to plot the metric data on to\n",
    "    metric: string\n",
    "        name of the metric being plotted\n",
    "    data: DataFrame\n",
    "        the data being plotted\n",
    "    '''\n",
    "\n",
    "    metric_info = {\n",
    "        'bg': ['line', 'Blood Glucose [mmol/L]', 'tab:red'], \n",
    "        'insulin': ['line', 'Insulin [U]', 'tab:blue'], \n",
    "        'carbs': ['scatter', 'Carbohydrate [grams]', 'tab:green'], \n",
    "        'hr': ['line', 'Heart Rate [bpm]', 'tab:pink'], \n",
    "        'dist': ['bar', 'Distance [m]', 'tab:gray'], \n",
    "        'steps': ['bar', 'Steps [count]', 'tab:cyan'], \n",
    "        'cals': ['line', 'Calories Burned [kcals]', 'tab:olive'],\n",
    "        'activity': ['scatter', 'Activity', 'tab:purple']\n",
    "    }\n",
    "\n",
    "    # filter the data to just be the plotted metric\n",
    "    metric_data = data[data[metric] == data[metric]].loc[:, [metric]]\n",
    "\n",
    "    # adds nan where the time difference between consecutive points exceeds 30 minutes to leave gaps in lines\n",
    "    if metric_info[metric][0] in ['line']:\n",
    "        time_diff = metric_data.index.to_series().diff()\n",
    "        gaps = time_diff > pd.Timedelta(minutes=30)\n",
    "        if gaps.any():\n",
    "            gap_starts = metric_data.index[gaps]\n",
    "            gap_ends = metric_data.index[gaps.shift(-1, fill_value=False)]\n",
    "            gap_midpoints = gap_starts + (gap_ends - gap_starts) / 2\n",
    "            nan_rows = pd.DataFrame(index=gap_midpoints)\n",
    "            metric_data = pd.concat([metric_data, nan_rows]).sort_index()\n",
    "    \n",
    "    # plot the data and format the y axis\n",
    "    ax.xaxis.set_major_formatter(md.DateFormatter('%m/%d %H:%M'))\n",
    "    ax.set_xlabel('Time [mm/dd HH:MM]')\n",
    "    ax.set_ylabel(metric_info[metric][1], color=metric_info[metric][2])\n",
    "    if metric_info[metric][0] == 'line':\n",
    "        ax.plot(\n",
    "            metric_data.index,\n",
    "            metric_data[metric],\n",
    "            color = metric_info[metric][2]\n",
    "        )\n",
    "    elif metric_info[metric][0] == 'scatter':\n",
    "        ax.scatter(\n",
    "            metric_data.index, \n",
    "            metric_data[metric],\n",
    "            color = metric_info[metric][2]\n",
    "        )\n",
    "    elif metric_info[metric][0] == 'bar':\n",
    "        ax.bar(\n",
    "            metric_data.index,\n",
    "            metric_data[metric],\n",
    "            width = pd.Timedelta(minutes=5),\n",
    "            color = metric_info[metric][2]\n",
    "        )\n",
    "    ax.tick_params(axis='y', labelcolor=metric_info[metric][2])\n",
    "    \n",
    "    \n",
    "def plot_axis(group, ax, data, start, end):\n",
    "    '''\n",
    "    Plots a subplot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group: list of strings\n",
    "        the group of metrics to be plotted on the axis\n",
    "    ax: matplotlib Axes\n",
    "        axis to plot the metric data on to\n",
    "    data: DataFrame\n",
    "        the data being plotted\n",
    "    start: string\n",
    "        lower bound for the date range to be plotted\n",
    "    end: string\n",
    "        upper bound for the date range to be plotted\n",
    "    '''\n",
    "    \n",
    "    # plots a line for each metric and adjust axis positions for each\n",
    "    for j, metric in enumerate(group):\n",
    "        if metric in ['bg', 'insulin', 'carbs', 'hr', 'dist', 'steps', 'cals', 'activity']:\n",
    "            if data[data[metric] == data[metric]][metric].empty:\n",
    "                print('No \"{}\" data in the plotted period.'.format(metric))\n",
    "            else:\n",
    "                if j > 0:\n",
    "                    ax_temp = ax.twinx()\n",
    "                    plot_line(ax_temp, metric, data)\n",
    "                    if j > 1:\n",
    "                        ax_temp.spines['right'].set_position(('outward', 60*(j-1)))\n",
    "                else:\n",
    "                    plot_line(ax, metric, data)\n",
    "        else:\n",
    "            print('\"{}\" is not an available metric to plot.'.format(metric))\n",
    "            \n",
    "    ax.set_xlim(pd.to_datetime(start), pd.to_datetime(end))\n",
    "        \n",
    "        \n",
    "def plot_data(data, start = '2023/07/01', end = '2023/07/04', layout = [['bg', 'insulin', 'carbs'], ['hr', 'dist', 'steps', 'cals', 'activity']]):\n",
    "    '''\n",
    "    Plots the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        participant data\n",
    "    start: string\n",
    "        lower bound for the date range to be plotted\n",
    "    end: string\n",
    "        upper bound for the date range to be plotted\n",
    "    layout: list of lists of strings\n",
    "        the metrics to plot on each axis\n",
    "    '''\n",
    "    data = data.loc[(data.index >= pd.to_datetime(start)) & (data.index < pd.to_datetime(end))]\n",
    "    \n",
    "    fig, ax = plt.subplots(len(layout), 1, figsize=(16,4*len(layout)))\n",
    "    \n",
    "    for i, group in enumerate(layout):\n",
    "        if len(layout) == 1:\n",
    "            plot_axis(group, ax, data, start, end)\n",
    "        else:\n",
    "            plot_axis(group, ax[i], data, start, end)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461f5e5",
   "metadata": {},
   "source": [
    "## Extract T1D data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e43046",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time(data):\n",
    "    '''\n",
    "    Breaks the data time period down into 5 minute chunks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        a dataframe containing the basal data in a column titled 'basal' and an \n",
    "        index with datetimes from that basal shift\n",
    "    '''\n",
    "    time_range = pd.date_range(\n",
    "        (\n",
    "            data[data['basal'].notna()].index.min() + \n",
    "            pd.Timedelta(minutes=2, seconds=29)\n",
    "        ).round('5min'), \n",
    "        (data.index.max()-pd.Timedelta(minutes=2, seconds=30)).round('5min'), \n",
    "        freq='5min'\n",
    "    )\n",
    "    \n",
    "    return time_range\n",
    "\n",
    "def add_device(p_path, data, group):\n",
    "    '''\n",
    "    Adds a column for the device the data has come from\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the devices file\n",
    "    data: DataFrame\n",
    "        data to add the column to\n",
    "    group: string\n",
    "        the device group that the device is from ('CGM', 'Insulin Pump' or \n",
    "        'Smartwatch')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        data with the added device column\n",
    "    '''\n",
    "    devices = pd.read_csv(os.path.join(p_path, 'devices.csv'))\n",
    "    devices['date'] = pd.to_datetime(devices['date'])\n",
    "    devices = devices.set_index('date')\n",
    "    \n",
    "    for time, device in devices[devices['group']==group].iterrows():\n",
    "        data.loc[data.index > time, 'device'] = device['device']\n",
    "        \n",
    "    return data\n",
    "\n",
    "def mean_duration(path):\n",
    "    '''\n",
    "    Finds the mean extended bolus duration across all participants.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to search for duration files in\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_dur: float\n",
    "        the mean duration from those foudn in the data\n",
    "    '''\n",
    "    durs = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if 'extended.csv' in files:\n",
    "            ext = pd.read_csv(os.path.join(root, 'extended.csv'))\n",
    "            durs = durs + pd.to_timedelta(\n",
    "                ext[ext['duration']==ext['duration']]['duration']+':00'\n",
    "            ).to_list()\n",
    "            \n",
    "    mean_dur = sum(durs, dt.timedelta(0)) / len(durs)\n",
    "    return mean_dur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d643708",
   "metadata": {},
   "source": [
    "### CareLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34731eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_carelink_pump(p_path, fso):\n",
    "    '''\n",
    "    Extracts data from carelink pump files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of file to extract the carelink pump data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        extracted insulin and carb values from the carelink pump file\n",
    "    '''\n",
    "    data = pd.read_csv(os.path.join(p_path, fso), low_memory=False)\n",
    "    data = data[[\n",
    "        'DateTime', \n",
    "        'Basal Rate (U/h)', \n",
    "        'Temp Basal Amount', \n",
    "        'Temp Basal Duration (h:mm:ss)', \n",
    "        'Bolus Volume Delivered (U)',\n",
    "        'Bolus Duration (h:mm:ss)',\n",
    "        'Suspend',\n",
    "        'BWZ Carb Input (grams)',\n",
    "        'BWZ Status',\n",
    "        'Bolus Source'\n",
    "    ]]\n",
    "    data = data.rename(columns={\n",
    "        'DateTime': 'timestamp', \n",
    "        'Basal Rate (U/h)': 'basal', \n",
    "        'Temp Basal Amount': 'temp_basal', \n",
    "        'Temp Basal Duration (h:mm:ss)': 'temp_basal_dur', \n",
    "        'Bolus Volume Delivered (U)': 'bolus',\n",
    "        'Bolus Duration (h:mm:ss)': 'bolus_dur',\n",
    "        'Suspend': 'suspend',\n",
    "        'BWZ Carb Input (grams)': 'carbs',\n",
    "        'BWZ Status': 'status',\n",
    "        'Bolus Source': 'source'\n",
    "    })\n",
    "    \n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.set_index('timestamp')\n",
    "    data = data.dropna(how='all')\n",
    "    data = data.iloc[::-1]\n",
    "\n",
    "    # adds the basal rate to rows highlighting return to normal pumping so that the basal rate change is repeated as with other basal rate changes\n",
    "    for time, row in data.iterrows():\n",
    "        if row['suspend'] == 'NORMAL_PUMPING':\n",
    "            data.at[time, 'basal'] = data[data['basal']==data['basal']]['basal'][time].mean()  \n",
    "\n",
    "    # when basal rate would change during a temporary basal rate replace the basal rate value with the temporary basal rate value\n",
    "    basal_rate = 0\n",
    "    temp_basals = data[data['temp_basal'] == data['temp_basal']][['temp_basal', 'temp_basal_dur']]\n",
    "\n",
    "    for time, temp_basal in temp_basals.iterrows():\n",
    "        count = 0\n",
    "        basal_events = data[(data.index >= time) & (data.index < time + pd.Timedelta(temp_basal['temp_basal_dur']))]\n",
    "\n",
    "        for t, event in basal_events[basal_events['basal'] == basal_events['basal']].iterrows():\n",
    "            if count%2 == 1:\n",
    "                data.at[t, 'basal'] = basal_rate\n",
    "            basal_rate = event['basal']\n",
    "            count += 1\n",
    "\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    # splits the time period into 5 minute intervals\n",
    "    time_range = split_time(data)\n",
    "    \n",
    "    # calculates the insulin delivered in the last 5 minutes for each time interval\n",
    "    insulin_values = []\n",
    "    basal_rate = data[data.index <= time_range[0]][data[data.index <= time_range[0]]['basal'].notna()]['basal'].iloc[-1]\n",
    "    basal_rate_temp = 0\n",
    "    basal_rate_temp_t = pd.to_datetime('2000/01/01')\n",
    "    for time in range(len(time_range)-1):\n",
    "        insulin = 0\n",
    "        recent_basal_t = time_range[time]\n",
    "        for index, event in data.loc[(data.index >= time_range[time]) & (data.index < time_range[time+1])].iterrows():\n",
    "            if event['basal'] == event['basal']:\n",
    "                if basal_rate_temp == event['basal'] and basal_rate_temp_t == (index+pd.Timedelta(seconds=29)).round('min'):\n",
    "                    insulin += (index - recent_basal_t).seconds/3600 * basal_rate\n",
    "                    basal_rate = event['basal']\n",
    "                    recent_basal_t = index\n",
    "                else:\n",
    "                    basal_rate_temp = event['basal']\n",
    "                    basal_rate_temp_t = (index+pd.Timedelta(seconds=29)).round('min')\n",
    "\n",
    "\n",
    "            if event['bolus'] == event['bolus']:\n",
    "                # if the insulin pump is in closed-loop mode each dose the insulin pump gives as part of the basal rate is recorded as a bolus instead so the basal needs to be ignored to avoid double counting\n",
    "                if event['source'] == 'CLOSED_LOOP_AUTO_BASAL' or event['source'] == 'CLOSED_LOOP_AUTO_BOLUS':\n",
    "                    basal_rate = 0\n",
    "\n",
    "                # retrospectively goes back and adds extended boluses\n",
    "                if event['bolus_dur'] == event['bolus_dur']:\n",
    "                    bolus_dur = pd.Timedelta(event['bolus_dur'])\n",
    "                    bolus_rate = event['bolus']/pd.Timedelta(event['bolus_dur']).seconds # in units per second\n",
    "\n",
    "                    if bolus_dur < index - (index-pd.Timedelta(minutes=2, seconds=31)).round('5min'):\n",
    "                        insulin += bolus_dur.seconds * bolus_rate\n",
    "                    else:\n",
    "                        insulin += (index - (index-pd.Timedelta(minutes=2, seconds=31)).round('5min')).seconds * bolus_rate\n",
    "                        bolus_dur -= index - (index-pd.Timedelta(minutes=2, seconds=31)).round('5min')\n",
    "                        i=1\n",
    "                        while bolus_dur >= pd.Timedelta(minutes=5):\n",
    "                            insulin_values[-i] += 300 * bolus_rate\n",
    "                            bolus_dur -= pd.Timedelta(minutes=5)\n",
    "                            i += 1\n",
    "                        insulin_values[-i] += bolus_dur.seconds * bolus_rate\n",
    "                else:\n",
    "                    insulin += event['bolus']\n",
    "        insulin += (time_range[time+1] - recent_basal_t).seconds/3600 * basal_rate\n",
    "        insulin_values.append(insulin)\n",
    "    \n",
    "    # formats the insulin values into a dataframe\n",
    "    insulin_values = pd.DataFrame({'timestamp': time_range[1:], 'insulin': insulin_values})\n",
    "    insulin_values = insulin_values.set_index('timestamp')\n",
    "\n",
    "    # removes carb values from cases where the delivery failed and 0 carbs is recorded and rounds them to the nearest minute\n",
    "    data = data.drop(data[data['status'] == 'Not Delivered'].index)\n",
    "    carbs = data[['carbs']]\n",
    "    carbs = carbs.loc[(carbs['carbs'] == carbs['carbs']) & (carbs['carbs'] > 0)]\n",
    "    carbs.index = carbs.index.round('min')\n",
    "    carbs = carbs.groupby(carbs.index).sum()\n",
    "\n",
    "    # combines insulin and carbs data and sorts\n",
    "    data = insulin_values.join(carbs, how='outer')\n",
    "    data = data.dropna(how='all')\n",
    "    data = data.sort_index()\n",
    "\n",
    "    # adds the device column\n",
    "    data = add_device(p_path, data, 'Insulin Pump')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_carelink_sensor(p_path, fso):\n",
    "    '''\n",
    "    Extracts data fropm carelink sensor files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of file to extract the carelink sensor data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        extracted bg values from the carelink sensor file\n",
    "    '''\n",
    "    data = pd.read_csv(os.path.join(p_path, fso), low_memory=False)\n",
    "    data = data[[\n",
    "        'DateTime',\n",
    "        'Sensor Glucose (mmol/L)'\n",
    "    ]]\n",
    "    data = data.rename(columns={\n",
    "        'DateTime': 'timestamp', \n",
    "        'Sensor Glucose (mmol/L)': 'bg'\n",
    "    })\n",
    "    data = data.drop_duplicates()\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.set_index('timestamp')\n",
    "    data.index = data.index.round('min')\n",
    "    data = data.dropna(how='all')\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    data = add_device(p_path, data, 'CGM')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d2270",
   "metadata": {},
   "source": [
    "### Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clarity(p_path, fso):\n",
    "    '''\n",
    "    Extracts bg readings from clairty files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of file to extract the clarity data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        extracted data from the clarity file\n",
    "    '''\n",
    "    data = pd.read_csv(os.path.join(p_path, fso))\n",
    "    data = data[[\n",
    "        'Timestamp',\n",
    "        'Glucose Value (mmol/L)'\n",
    "    ]]\n",
    "    data = data.rename(columns={\n",
    "        'Timestamp': 'timestamp',\n",
    "        'Glucose Value (mmol/L)': 'bg'\n",
    "    })\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # sets high and low warnings to the threshold values\n",
    "    data['bg'] = data['bg'].replace('Low', '2.2')\n",
    "    data['bg'] = data['bg'].replace('High', '22.2')\n",
    "    data['bg'] = pd.to_numeric(data['bg'])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp']).round('min')\n",
    "    data = data.set_index('timestamp')\n",
    "    \n",
    "    data = data.sort_index()\n",
    "    data = add_device(p_path, data, 'CGM')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca787b0",
   "metadata": {},
   "source": [
    "### Glooko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_misalignment(cgm_data):\n",
    "    '''\n",
    "    Detects misalignment between cgm readings recorded on the pump and phone and adjusts the cgm readings on the pump based on these misalignments\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cgm_data: DataFrame\n",
    "        glooko cgm data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cgm_data: DataFrame\n",
    "        cgm data updated for mislaingments\n",
    "    misalignment_start_end: DataFrame\n",
    "        the start end and value of misalignments detected in the cgm readings\n",
    "    '''\n",
    "    misalignment_start_end = pd.DataFrame()\n",
    "    \n",
    "    if 'pump' in cgm_data['source'].unique() and 'phone' in cgm_data['source'].unique():\n",
    "        pump_data = cgm_data[cgm_data['source'] == 'pump'].reset_index(drop=True)\n",
    "        phone_data = cgm_data[cgm_data['source'] == 'phone'].reset_index(drop=True)\n",
    "\n",
    "        # calculates all possible misalignments\n",
    "        misalignments = []\n",
    "        for i, pump_row in pump_data.iterrows():\n",
    "            pump_time = pump_row['timestamp']\n",
    "            pump_value = pump_row['bg']\n",
    "            misalignments.append((pump_time - phone_data[(phone_data['timestamp'] > pump_time - pd.Timedelta(hours=2)) & (phone_data['timestamp'] < pump_time + pd.Timedelta(hours=2)) & (phone_data['bg'] == pump_value)]['timestamp']).to_list())\n",
    "        pump_data['misalignments'] = misalignments\n",
    "\n",
    "        # selects frequently occuring misalingments as the true misalingment\n",
    "        final_misalignments = []\n",
    "        for i, pump_row in pump_data.iterrows():\n",
    "            final_misalignment = pd.NaT\n",
    "            for misalignment in pump_row['misalignments']:\n",
    "                count = 0\n",
    "                for other_misalignments in pump_data[(pump_data['timestamp'] > pump_row['timestamp']) & (pump_data['timestamp'] < pump_row['timestamp'] + pd.Timedelta(minutes=90))]['misalignments']:\n",
    "                    if misalignment in other_misalignments:\n",
    "                        count += 1\n",
    "                if count > 15:\n",
    "                    final_misalignment = misalignment\n",
    "                    break\n",
    "                count = 0\n",
    "                for other_misalignments in pump_data[(pump_data['timestamp'] > pump_row['timestamp'] - pd.Timedelta(minutes=90)) & (pump_data['timestamp'] < pump_row['timestamp'])]['misalignments']:\n",
    "                    if misalignment in other_misalignments:\n",
    "                        count += 1\n",
    "                if count > 15:\n",
    "                    final_misalignment = misalignment\n",
    "                    break\n",
    "            final_misalignments.append(final_misalignment)\n",
    "        pump_data['misalignments'] = final_misalignments\n",
    "\n",
    "        if len(pump_data[pump_data['misalignments'] == pump_data['misalignments']]['misalignments']) > 0:\n",
    "            # groups the misalignments into groups with start and end times\n",
    "            misalignment_data = pump_data[pump_data['misalignments'] == pump_data['misalignments']][['timestamp', 'misalignments']]\n",
    "            mask = (misalignment_data['misalignments'] == misalignment_data['misalignments'].shift(1)) & (misalignment_data['misalignments'] == misalignment_data['misalignments'].shift(-1))\n",
    "            misalignment_data = misalignment_data[~mask]\n",
    "            misalignment_data['group'] = (misalignment_data['misalignments'] != misalignment_data['misalignments'].shift()).cumsum()\n",
    "            misalignment_start_end = (\n",
    "                misalignment_data.groupby('group')\n",
    "                .agg(\n",
    "                    start_time=('timestamp', 'first'),\n",
    "                    end_time=('timestamp', 'last'),\n",
    "                    misalignment=('misalignments', 'first')\n",
    "                )\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # extends the misalingment time ranges to be touching if there is a small change in misalingment value\n",
    "            for i in range(len(misalignment_start_end) - 1):\n",
    "                if np.abs(misalignment_start_end.loc[i+1, 'misalignment'] - misalignment_start_end.loc[i, 'misalignment']) <= pd.Timedelta(minutes=3):\n",
    "                    misalignment_start_end.loc[i, 'end_time'] = misalignment_start_end.loc[i+1, 'start_time']\n",
    "\n",
    "            # updates pump cgm readings based on the misalingments\n",
    "            for i, misalignment in misalignment_start_end.iterrows():\n",
    "                pump_data.loc[(pump_data['timestamp'] >= misalignment['start_time']) & (pump_data['timestamp'] < misalignment['end_time']), 'timestamp'] = pump_data['timestamp'] - misalignment['misalignment']\n",
    "            \n",
    "            # remove phone readings when there are reliable pump values within 5 minutes\n",
    "            for i, pump_row in pump_data.iterrows():\n",
    "                pump_time = pump_row['timestamp']\n",
    "                if len(pump_data[(pump_data['timestamp'] >= pump_time - pd.Timedelta(minutes=15)) & (pump_data['timestamp'] <= pump_time + pd.Timedelta(minutes=15))]) > 2:\n",
    "                    phone_data = phone_data.drop(phone_data[(phone_data['timestamp'] > pump_time - pd.Timedelta(minutes=5)) & (phone_data['timestamp'] < pump_time + pd.Timedelta(minutes=5))].index)\n",
    "\n",
    "            cgm_data = pd.concat([pump_data, phone_data])\n",
    "\n",
    "        cgm_data = cgm_data[['timestamp', 'bg']]\n",
    "\n",
    "    return cgm_data, misalignment_start_end\n",
    "    \n",
    "def correct_insulin_misalignment(insulin, misalignment_start_end):\n",
    "    '''\n",
    "    Adjusts insulin timestamps based on misalignment found between pump and phone cgm readings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    insulin: DataFrame\n",
    "        basal or bolus data\n",
    "    misalignment_start_end: DataFrame\n",
    "        misalignment times and values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    insulin: DataFrame\n",
    "        basal or bolus data adjusted by misalignment times\n",
    "    '''\n",
    "    for i, misalignment in misalignment_start_end.iterrows():\n",
    "        insulin.loc[misalignment['start_time']:misalignment['end_time']].index - misalignment['misalignment']\n",
    "            \n",
    "    return insulin\n",
    "\n",
    "def extract_glooko(p_path, fsos, omnipod_5_start = None):\n",
    "    '''\n",
    "    Extracts data from glooko files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fsos: list of string\n",
    "        names of directory to extract the glooko data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        extracted data from the glooko files\n",
    "    '''\n",
    "    data = pd.DataFrame()\n",
    "    cgm = pd.DataFrame()\n",
    "    misalignment_start_end = pd.DataFrame()\n",
    "\n",
    "    # loads and joins all the cgm data\n",
    "    for fso in fsos:\n",
    "        path_glooko = os.path.join(p_path, fso)\n",
    "        if 'cgm_data.csv' in os.listdir(path_glooko):\n",
    "            cgm = pd.concat([cgm, pd.read_csv(os.path.join(path_glooko, 'cgm_data.csv'))])\n",
    "\n",
    "    # extracts the cgm data if there is any\n",
    "    if not cgm.empty:\n",
    "        cgm = cgm.drop_duplicates().reset_index(drop=True)\n",
    "        cgm = cgm.rename(columns = {'Timestamp': 'timestamp', 'CGM Glucose Value (mmol/L)': 'bg', 'Source': 'source'})\n",
    "        cgm['bg'] = cgm['bg'].replace(0.1, 2.2)\n",
    "        cgm['bg'] = cgm['bg'].replace(111.1, 22.2)\n",
    "        cgm['timestamp'] = pd.to_datetime(cgm['timestamp'])\n",
    "\n",
    "        cgm, misalignment_start_end = detect_misalignment(cgm)\n",
    "\n",
    "        cgm = cgm.set_index('timestamp')\n",
    "        cgm = cgm[['bg']]\n",
    "        cgm = add_device(p_path, cgm, 'CGM')\n",
    "        data = data.join(cgm, how='outer')\n",
    "\n",
    "    # extracts the insulin data\n",
    "    for fso in fsos:\n",
    "        pump = pd.DataFrame()\n",
    "        path_glooko = os.path.join(p_path, fso)\n",
    "        if 'basal_data.csv' in os.listdir(path_glooko) and 'bolus_data.csv' in os.listdir(path_glooko):\n",
    "            # reads the basal data and formats it\n",
    "            basal = pd.read_csv(os.path.join(path_glooko, 'basal_data.csv'))\n",
    "            basal = basal[['Timestamp', 'Duration (minutes)', 'Rate']]\n",
    "            basal = basal.rename(columns = {'Timestamp': 'timestamp', 'Duration (minutes)': 'duration', 'Rate': 'basal'})\n",
    "            basal['timestamp'] = pd.to_datetime(basal['timestamp'])\n",
    "            basal = basal.set_index('timestamp').sort_index()\n",
    "            \n",
    "            # reads the bolus data and formats it\n",
    "            bolus = pd.read_csv(os.path.join(path_glooko, 'bolus_data.csv'))\n",
    "            bolus = bolus[[\n",
    "                'Timestamp', \n",
    "                'Carbs input (g)', \n",
    "                'Insulin delivered (U)', \n",
    "                'Initial delivery (U)', \n",
    "                'Extended delivery (U)'\n",
    "            ]]\n",
    "            bolus = bolus.rename(columns = {\n",
    "                'Timestamp': 'timestamp', \n",
    "                'Carbs input (g)': 'carbs', \n",
    "                'Insulin delivered (U)': 'bolus', \n",
    "                'Initial delivery (U)': 'instant', \n",
    "                'Extended delivery (U)': 'extended'\n",
    "            })\n",
    "            bolus['timestamp'] = pd.to_datetime(bolus['timestamp'])\n",
    "            bolus = bolus.set_index('timestamp').sort_index()\n",
    "\n",
    "            # adds extended bolus durations in the cases they exist\n",
    "            if 'extended.csv' in os.listdir(p_path):\n",
    "                duration = pd.read_csv(os.path.join(p_path, 'extended.csv'))\n",
    "                duration['timestamp'] = pd.to_datetime(duration['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "                duration['duration'] = pd.to_timedelta(duration['duration']+':00')\n",
    "                duration = duration.set_index('timestamp')\n",
    "                \n",
    "            for index, event in bolus.iterrows():\n",
    "                if event['extended'] == event['extended'] and event['extended'] > 0:\n",
    "                    try:\n",
    "                        bolus.at[index, 'duration'] = duration['duration'][index]\n",
    "                    except:\n",
    "                        # if there isn't extended bolus durations collected (P13 and P23) uses the mean extended bolus duration across the other participants instead rounded to the nearest minute (1:39:00)\n",
    "                        bolus.at[index, 'duration'] = pd.to_timedelta('01:39:00')\n",
    "                        print('missing duration value so the default 1:39:00 is used')\n",
    "                else:\n",
    "                    bolus.at[index, 'instant'] = event['bolus']\n",
    "                    bolus.at[index, 'extended'] = 0.0\n",
    "\n",
    "            if len(misalignment_start_end) > 0:\n",
    "                basal = correct_insulin_misalignment(basal, misalignment_start_end)\n",
    "                bolus = correct_insulin_misalignment(bolus, misalignment_start_end)\n",
    "\n",
    "            # splits the time period into 5 minute chunks\n",
    "            time_range = split_time(basal)\n",
    "            \n",
    "            # caluclates the insulin delivered in the last 5 minutes for each time chunk\n",
    "            insulin_values = [0] * (len(time_range)-1)\n",
    "            basal_rate = basal[basal.index <= time_range[0]]['basal'].iloc[-1]\n",
    "            for i in range(len(insulin_values)):\n",
    "                recent_basal_t = time_range[i]\n",
    "                interval_basals = basal.loc[(basal.index >= time_range[i]) & (basal.index < time_range[i+1])]\n",
    "\n",
    "                for index, event in interval_basals.iterrows():\n",
    "                    if len(interval_basals.loc[[index]]) == 1 or event['duration'] > 1:\n",
    "                        insulin_values[i] += (index - recent_basal_t).seconds/3600 * basal_rate\n",
    "                        basal_rate = event['basal']\n",
    "                        recent_basal_t = index\n",
    "                \n",
    "                for index, event in bolus.loc[(bolus.index >= time_range[i]) & (bolus.index < time_range[i+1])].iterrows():\n",
    "                    insulin_values[i] += event['instant']\n",
    "                    if event['extended'] > 0:\n",
    "                        bolus_dur = event['duration']\n",
    "                        bolus_rate = event['extended']/event['duration'].seconds # in units per second\n",
    "                        insulin_values[i] += (index - (index-pd.Timedelta(minutes=2, seconds=31)).round('5min')).seconds * bolus_rate\n",
    "                        bolus_dur -= (index - (index-pd.Timedelta(minutes=2, seconds=31)).round('5min'))\n",
    "                        j=1\n",
    "                        while bolus_dur >= pd.Timedelta(minutes=5):\n",
    "                            insulin_values[i+j] += 300 * bolus_rate\n",
    "                            bolus_dur -= pd.Timedelta(minutes=5)\n",
    "                            j += 1\n",
    "                        insulin_values[i+j] += bolus_dur.seconds * bolus_rate\n",
    "\n",
    "                insulin_values[i] += (time_range[i+1] - recent_basal_t).seconds/3600 * basal_rate\n",
    "            \n",
    "            # formats the insulin values into a dataframe and adds the device\n",
    "            insulin_values = pd.DataFrame({'timestamp': time_range[1:], 'insulin': insulin_values})\n",
    "            insulin_values = insulin_values.set_index('timestamp')\n",
    "\n",
    "            # excludes insulin data recorded on an omnipod 5\n",
    "            if omnipod_5_start:\n",
    "                pump = insulin_values[insulin_values.index < omnipod_5_start].copy()\n",
    "            else:\n",
    "                pump = insulin_values.copy()\n",
    "\n",
    "            # assume if no bolus devliered with carbs that it was a failed delivery\n",
    "            carbs = bolus.loc[(bolus['carbs'] == bolus['carbs']) & (bolus['carbs'] > 0) & (bolus['bolus'] > 0)][['carbs']]\n",
    "            if pump.empty:\n",
    "                pump = carbs.copy()\n",
    "            else:\n",
    "                pump = pump.join(carbs, how='outer')\n",
    "\n",
    "            pump = add_device(p_path, pump, 'Insulin Pump')\n",
    "\n",
    "        data = pd.concat([data, pump])\n",
    "\n",
    "    # reorder data and remove duplciates\n",
    "    data = data.reset_index().drop_duplicates().set_index('timestamp')\n",
    "    data = data.sort_index()\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa23759",
   "metadata": {},
   "source": [
    "### LibreView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3082672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_libreview(p_path, fso):\n",
    "    '''\n",
    "    Extracts data from libreview files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of file to extract the libreview data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        extracted bg readings from the libreview file\n",
    "    '''\n",
    "    data = pd.read_csv(os.path.join(p_path, fso))\n",
    "    data = data[data['Record Type'] == 0]\n",
    "    data = data[['Device Timestamp', 'Historic Glucose mmol/L']]\n",
    "    data = data.rename(columns = {'Device Timestamp': 'timestamp', 'Historic Glucose mmol/L': 'bg'})\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.set_index('timestamp')\n",
    "    data = add_device(p_path, data, 'CGM')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc0592",
   "metadata": {},
   "source": [
    "## Extract smartwatch data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ce6bc",
   "metadata": {},
   "source": [
    "### Fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ae234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_takeout(p_path, fso):\n",
    "    '''\n",
    "    Extracts data from takeout fitbit files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of folder to extract the takeout data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        data set of the extracted smartwatch data from the takeout files\n",
    "    '''\n",
    "    path_takeout = os.path.join(p_path, fso, 'Global Export Data')\n",
    "    \n",
    "    data = extract_fitbit(p_path, path_takeout)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "def extract_dashboard(p_path, fso):\n",
    "    '''\n",
    "    Extracts data from fitbit dashboard files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of folder to extract the fitbit dashboard data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        data set of the extracted smartwatch data from the fitbit dashboard files\n",
    "    '''\n",
    "    path_dashboard = os.path.join(p_path, fso, 'Physical Activity')\n",
    "    \n",
    "    data = extract_fitbit(p_path, path_dashboard)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_timezones(path):\n",
    "    '''\n",
    "    Extracts any timezone changes from the heart rate files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the directory with the heart rate data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    time_diffs: DataFrame\n",
    "        a dataframe of the timezone changes and when they are first detected plus the starting timezone and time change out of BST\n",
    "    '''\n",
    "    time_diffs = []\n",
    "    \n",
    "    # searches through the heart rate files to look for exact hour differences between the first or last reading and the start and end of the day\n",
    "    for file in os.listdir(path):\n",
    "        if file.startswith('heart_rate-'):\n",
    "            file_date = dt.datetime.strptime(file[11:21], '%Y-%m-%d') \n",
    "            \n",
    "            f = open(os.path.join(path, file))\n",
    "            hr = json.load(f)\n",
    "            \n",
    "            first_time = dt.datetime.strptime(hr[0]['dateTime'], '%m/%d/%y %H:%M:%S')\n",
    "            last_time = dt.datetime.strptime(hr[-1]['dateTime'], '%m/%d/%y %H:%M:%S')\n",
    "            \n",
    "            if first_time.minute == 0:\n",
    "                time_diff = dt.timedelta(hours=round((file_date - first_time).total_seconds() / 3600))\n",
    "                time_diffs.append({'date': file_date, 'time_diff': time_diff})\n",
    "                \n",
    "            elif last_time.minute == 59:\n",
    "                time_diff = dt.timedelta(hours=round((file_date + dt.timedelta(days=1) - last_time).total_seconds() / 3600))\n",
    "                time_diffs.append({'date': file_date, 'time_diff': time_diff})\n",
    "            \n",
    "            f.close()\n",
    "\n",
    "    # formats time differences and removes anomalous time changes\n",
    "    time_diffs = pd.DataFrame(time_diffs)\n",
    "    time_diffs = time_diffs.sort_values(by = 'date')\n",
    "    consecutive_mask = (time_diffs['time_diff'] == time_diffs['time_diff'].shift(1)) | (time_diffs['time_diff'] == time_diffs['time_diff'].shift(-1))\n",
    "    time_diffs = time_diffs[consecutive_mask]\n",
    "    \n",
    "    # adds starting time zones and UK time zone shift\n",
    "    uk_time = pd.DataFrame({\n",
    "        'date': [\n",
    "            dt.datetime.strptime('2023-05-31 00:00:00', '%Y-%m-%d %H:%M:%S'), \n",
    "            dt.datetime.strptime('2023-10-29 02:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "        ], \n",
    "        'time_diff': [\n",
    "            dt.timedelta(hours=1),\n",
    "            dt.timedelta(hours=0)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    time_diffs = pd.concat([time_diffs, uk_time], ignore_index=True)\n",
    "    time_diffs = time_diffs.sort_values(by = 'date')\n",
    "    \n",
    "    time_diffs = time_diffs[time_diffs['time_diff'] != time_diffs['time_diff'].shift(1)]\n",
    "    \n",
    "    return time_diffs\n",
    "\n",
    "\n",
    "def convert_to_local_time(time_diff, data):\n",
    "    '''\n",
    "    Converts the times of data from GMT to local time using timezone information\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_diff: DataFrame\n",
    "        the timezone changes and when they occur\n",
    "        \n",
    "    data: DataFrame\n",
    "        the data to have it's time updated\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merged_data: DataFrame\n",
    "        the data with updates times based on timezone difference\n",
    "    '''\n",
    "    # format the data\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    time_diff['date'] = pd.to_datetime(time_diff['date'])\n",
    "    data = data.sort_values('timestamp')\n",
    "    time_diff = time_diff.sort_values('date')\n",
    "\n",
    "    # adjust by timezone\n",
    "    merged_data = pd.merge_asof(data, time_diff, left_on='timestamp', right_on='date', direction='backward')\n",
    "    merged_data['timestamp'] = merged_data.apply(\n",
    "        lambda row: (row['timestamp'] + row['time_diff']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # remove uneeded columns\n",
    "    merged_data = merged_data.drop(columns=['date', 'time_diff'])\n",
    "    merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def extract_json(path, date_key, value_keys, value_transforms):\n",
    "    '''\n",
    "    Extarct the data from json files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the json file\n",
    "    data_key: string\n",
    "        index of the timestamp data\n",
    "    value_keys: list of strings\n",
    "        the index of the data\n",
    "    value_transforms: list of string methods\n",
    "        the methods to use to format the data\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    processed_data: list of list\n",
    "        the extarcted data\n",
    "    '''\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        processed_data = [[dt.datetime.strptime(item[date_key], '%m/%d/%y %H:%M:%S')] + [transform(item[key]) for key, transform in zip(value_keys, value_transforms)] for item in data]\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def extract_fitbit_metric(data, time_diff, columns, method):\n",
    "    '''\n",
    "    Extract a metric from the extracted json files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list of lists\n",
    "        the data extracted fromthe json files\n",
    "    time_diff: DataFrame\n",
    "        the time differences to adjust the values by\n",
    "    columns: list of strings\n",
    "        the column names of the data\n",
    "    method: string\n",
    "        aggregation method, either 'mean' or 'sum'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metric: DataFrame\n",
    "        the extracted metric\n",
    "    '''\n",
    "    # format the data and adjust the timezones\n",
    "    metric = pd.DataFrame(data, columns=columns)\n",
    "    metric = convert_to_local_time(time_diff, metric)\n",
    "    metric = metric.set_index('timestamp')\n",
    "    metric = metric.sort_index()\n",
    "    \n",
    "    # resample the data to 5 minute intervals, aggregated based on the method inputted\n",
    "    resampled_count = metric.resample('5min', closed='right', label='right').count()\n",
    "    if method == 'mean':\n",
    "        metric = metric.resample('5min', closed='right', label='right').mean()\n",
    "    elif method == 'sum':\n",
    "        metric = metric.resample('5min', closed='right', label='right').sum()\n",
    "    metric[resampled_count == 0] = float('nan')\n",
    "    \n",
    "    return metric\n",
    "\n",
    "def extract_fitbit_activities(exe, time_diff):\n",
    "    '''\n",
    "    Extracts the activity labels from the exercise data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    exe: list of lists\n",
    "        the data from the exercise jsons\n",
    "    time_diff: DataFrame\n",
    "        the time differences to adjust the values by\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    act: DataFrame\n",
    "        the extarcted activity labels\n",
    "    '''\n",
    "    # formats the data\n",
    "    exe = pd.DataFrame(exe, columns=['timestamp', 'activity', 'duration'])\n",
    "    exe = convert_to_local_time(time_diff, exe)\n",
    "    exe['timestamp'] = pd.to_datetime(exe['timestamp'])\n",
    "    \n",
    "    # adds the end of activity\n",
    "    exe['end'] = exe['timestamp'] + exe['duration']\n",
    "    \n",
    "    # round the start and end times to the nearest 5 minutes\n",
    "    exe['start'] = (exe['timestamp'] + pd.Timedelta(seconds=450)).dt.floor('5min')\n",
    "    exe['end'] = (exe['end'] - pd.Timedelta(seconds=150)).dt.ceil('5min')\n",
    "    \n",
    "    # resample the data into 5 minute intervals and add the activity label to intervals within activity events\n",
    "    act = pd.DataFrame()\n",
    "    for i, row in exe.iterrows():\n",
    "        act_range = pd.date_range(row['start'], row['end'], freq='5min')\n",
    "        act = pd.concat([act, pd.DataFrame({'activity': [row['activity']] * len(act_range)}, index=act_range)])\n",
    "    act = act[~act.index.duplicated(keep='last')]\n",
    "    act.index.name = 'timestamp'\n",
    "    \n",
    "    return act\n",
    "\n",
    "def extract_fitbit(p_path, d_path):\n",
    "    '''\n",
    "    Extracts data from fitbit files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    d_path: string\n",
    "        path to the participant's general pa data directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        data set of the extracted smartwatch data from the takeout files\n",
    "    '''\n",
    "    \n",
    "    # generate lists to collect data in\n",
    "    hr = []\n",
    "    dist = []\n",
    "    steps = []\n",
    "    cals = []\n",
    "    exe = []\n",
    "    \n",
    "    # extract the json data\n",
    "    for file in os.listdir(d_path):\n",
    "        f_path = os.path.join(d_path, file)\n",
    "        if file.startswith('heart_rate-'):\n",
    "            hr += extract_json(f_path, 'dateTime', ['value'], [lambda x: x['bpm']])\n",
    "        elif file.startswith('distance-'):\n",
    "            dist += extract_json(f_path, 'dateTime', ['value'], [lambda x: int(x)/100])\n",
    "        elif file.startswith('steps-'):\n",
    "            steps += extract_json(f_path, 'dateTime', ['value'], [int])\n",
    "        elif file.startswith('calories-'):\n",
    "            cals += extract_json(f_path, 'dateTime', ['value'], [float])\n",
    "        elif file.startswith('exercise-'):\n",
    "            exe += extract_json(f_path, 'startTime', ['activityName', 'duration'], [str, lambda x: dt.timedelta(milliseconds=x)])\n",
    "    \n",
    "    # calculate the timezones\n",
    "    time_diff = extract_timezones(d_path)\n",
    "    \n",
    "    # format and aggregate the metric data\n",
    "    hr = extract_fitbit_metric(hr, time_diff, ['timestamp', 'hr'], 'mean')\n",
    "    dist = extract_fitbit_metric(dist, time_diff, ['timestamp', 'dist'], 'sum')\n",
    "    steps = extract_fitbit_metric(steps, time_diff, ['timestamp', 'steps'], 'sum')\n",
    "    cals = extract_fitbit_metric(cals, time_diff, ['timestamp', 'cals'], 'sum')\n",
    "    act = extract_fitbit_activities(exe, time_diff)\n",
    "    \n",
    "    # combine metrics into a single dataframe and adds the device name    \n",
    "    data = pd.concat([hr, dist, steps, cals, act], axis=1)\n",
    "    data = add_device(p_path, data, 'Smartwatch')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46d261",
   "metadata": {},
   "source": [
    "### Apple Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tz(path):\n",
    "    '''\n",
    "    Extract the timezone data from the tz.csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the tz.csv file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tz: DataFrame\n",
    "        a dataframe of the timezone data\n",
    "    '''\n",
    "    # open and format the timezone data\n",
    "    tz = pd.read_csv(os.path.join(path, 'tz.csv'))\n",
    "    tz['timestamp'] = pd.to_datetime(tz['timestamp'])\n",
    "    tz['timestamp'] = tz['timestamp'].dt.tz_localize(None)\n",
    "    \n",
    "    # add the timezone for the start of the data collection period and sort it\n",
    "    start_time = pd.DataFrame({'timestamp': [dt.datetime.strptime('2023-05-31 00:00:00', '%Y-%m-%d %H:%M:%S')], 'tz': ['Europe/London']})\n",
    "    tz = pd.concat([tz, start_time], ignore_index=True)\n",
    "    tz = tz.sort_values('timestamp')\n",
    "    \n",
    "    # only includes the instances when the timezone first changes\n",
    "    tz = tz[tz['tz'] != tz['tz'].shift(1)]\n",
    "\n",
    "    return tz\n",
    "\n",
    "\n",
    "def extract_record(path):\n",
    "    '''\n",
    "    Extract the activity data from the record.csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the record.csv file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    record: DataFrame\n",
    "        a dataframe of the activity data\n",
    "    '''\n",
    "    # opens and formats the activty data, removing the UTC timezone\n",
    "    record = pd.read_csv(os.path.join(path, 'record.csv'), low_memory=False)\n",
    "    record = record[[\n",
    "        'type',\n",
    "        'unit',\n",
    "        'endDate',\n",
    "        'value',\n",
    "        'device'\n",
    "    ]]\n",
    "    record['endDate'] = pd.to_datetime(record['endDate'])\n",
    "    record['endDate'] = record['endDate'].dt.tz_localize(None)\n",
    "    record = record.sort_values('endDate')\n",
    "    \n",
    "    return record\n",
    "\n",
    "\n",
    "def extract_workout(path):\n",
    "    '''\n",
    "    Extract the workout data from the workout.csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the workout.csv file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    workout: DataFrame\n",
    "        a dataframe of the workout data\n",
    "    '''\n",
    "    # opens and formats the workout data\n",
    "    workout = pd.read_csv(os.path.join(path, 'workout.csv'))\n",
    "    workout = workout[[\n",
    "        'workoutActivityType',\n",
    "        'startDate',\n",
    "        'endDate',\n",
    "        'device'\n",
    "    ]]\n",
    "    workout['startDate'] = pd.to_datetime(workout['startDate'])\n",
    "    workout['startDate'] = workout['startDate'].dt.tz_localize(None)\n",
    "    workout['endDate'] = pd.to_datetime(workout['endDate'])\n",
    "    workout['endDate'] = workout['endDate'].dt.tz_localize(None)\n",
    "    \n",
    "    # removes 'HKWorkoutActivityType' from activity labels\n",
    "    workout['workoutActivityType'] = workout['workoutActivityType'].apply(lambda x: x[21:])\n",
    "    \n",
    "    return workout\n",
    "\n",
    "def extract_bpm(path):\n",
    "    '''\n",
    "    Extract the heart rate data from the bpm.csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the bpm.csv file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bpm: DataFrame\n",
    "        a dataframe of the bpm data\n",
    "    '''\n",
    "    # opens and formats the bpm data\n",
    "    bpm = pd.read_csv(os.path.join(path, 'bpm.csv'))\n",
    "    bpm['time'] = pd.to_datetime(bpm['time'])\n",
    "    bpm['time'] = bpm['time'].dt.tz_localize(None)\n",
    "    bpm = bpm.sort_values('time')\n",
    "    \n",
    "    return bpm\n",
    "\n",
    "\n",
    "def adjust_timezone(data, tz, time_columns):\n",
    "    '''\n",
    "    Adjusts the timestamps in the data according to the timezones\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    data: DataFrame\n",
    "        the data to adjust the timestamps of\n",
    "    tz: DataFrame\n",
    "        the timezones shifts to update the data by\n",
    "    time_columns: list of strings\n",
    "        the names of the columns of timestamps in the data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merged_data: DataFrame\n",
    "        the data with the timestamps adjusted\n",
    "    '''\n",
    "    # merge the data and corresponding timezones\n",
    "    merged_data = pd.merge_asof(data, tz, left_on=time_columns[0], right_on='timestamp', direction='backward')\n",
    "\n",
    "    # adjust the timestamps with the corresponding timezone\n",
    "    for time_column in time_columns:\n",
    "        merged_data[time_column] = merged_data.apply(\n",
    "            lambda row: row[time_column].tz_localize(pytz.utc).tz_convert(row['tz']).tz_localize(None),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # remove the timezone columns\n",
    "    merged_data = merged_data.drop(columns = ['timestamp', 'tz'])\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def extract_apple_metric(record, metric_id, metric_name, method, extra_data = []):\n",
    "    '''\n",
    "    Extracts a metric from the record.csv file and aggregates it into 5-minute chunks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    record: DataFrame\n",
    "        the record.csv file\n",
    "    metric_id: string\n",
    "        the type of the metric being extarcted\n",
    "    metric_name: string\n",
    "        the name of the metric\n",
    "    method: DataFrame method\n",
    "        how the date is aggregated\n",
    "    extra_data: list of DataFrames\n",
    "        any extra data to be added before aggregation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    metric: DataFrame\n",
    "        the aggregated metric\n",
    "    '''\n",
    "    # extract chosen metric\n",
    "    metric = record.loc[(record['type'] == metric_id) & (record['device'].str.contains('Watch'))]\n",
    "    metric.loc[:,'value'] = pd.to_numeric(metric.loc[:,'value'])\n",
    "\n",
    "    # correct units to count/min, km, count, kcal if not already\n",
    "    unit_conversions = {\n",
    "         'count/min': 1,\n",
    "         'km': 1,\n",
    "         'count': 1,\n",
    "         'kcal': 1,\n",
    "         'mi': 1.609344,\n",
    "         'Cal': 1\n",
    "    }\n",
    "    for unit in metric['unit'].unique():\n",
    "        if unit not in unit_conversions:\n",
    "            print('Unknown unit:', unit)\n",
    "        else:\n",
    "            metric.loc[metric['unit'] == unit, 'value'] *= unit_conversions[unit]\n",
    "\n",
    "    # format dataframe\n",
    "    metric = metric[['endDate', 'value']]\n",
    "    metric = metric.rename(columns = {'endDate': 'timestamp', 'value': metric_name})\n",
    "    \n",
    "    # add any extra data and sort it\n",
    "    for data in extra_data:\n",
    "        metric = pd.concat([metric, data])\n",
    "    \n",
    "    metric = metric.set_index('timestamp')\n",
    "    metric = metric.sort_index()\n",
    "    \n",
    "    # resample the data at 5 minute intervals and apply the method to values in this interval then set the value to nan for any intervals with no values\n",
    "    resampled_count = metric.resample('5min', closed='right', label='right').count()\n",
    "    metric = metric.resample('5min', closed='right', label='right').apply(method)\n",
    "    metric[resampled_count == 0] = float('nan')\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "def extract_apple_activities(workout):\n",
    "    '''\n",
    "    Extracts the activity name data from the workout.csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    workout: DataFrame\n",
    "        the workout data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    act: DataFrame\n",
    "        the extarcted activity name data\n",
    "    '''\n",
    "    # filter entries made by the smartwatch\n",
    "    workout = workout[workout['device'].str.contains('Watch').fillna(False)]\n",
    "    workout = workout.drop(columns=['device'])\n",
    "    \n",
    "    # format the data and round the start and end times\n",
    "    workout = workout.rename(columns={\n",
    "        'workoutActivityType': 'activity',\n",
    "        'startDate': 'start',\n",
    "        'endDate': 'end'\n",
    "    })\n",
    "    workout['start'] = (workout['start'] + pd.Timedelta(seconds=450)).dt.floor('5min')\n",
    "    workout['end'] = (workout['end'] - pd.Timedelta(seconds=150)).dt.ceil('5min')\n",
    "    \n",
    "    # generate a dataframe of every 5 minute interval that activity was being performed in with the activity label\n",
    "    act = pd.DataFrame()\n",
    "    for i, row in workout.iterrows():\n",
    "        act_range = pd.date_range(row['start'], row['end'], freq='5min')\n",
    "        act = pd.concat([act, pd.DataFrame({'activity': [row['activity']] * len(act_range)}, index=act_range)])\n",
    "    \n",
    "    act = act.rename_axis('timestamp')\n",
    "\n",
    "    return act\n",
    "\n",
    "\n",
    "def extract_apple(p_path, fso):\n",
    "    '''\n",
    "    Extracts data from apple health files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_path: string\n",
    "        path to the participant's directory\n",
    "    fso: string\n",
    "        name of folder to extract the apple health data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: DataFrame\n",
    "        data set of the extracted smartwatch data from the apple health files\n",
    "    '''\n",
    "    # get timezone data\n",
    "    tz = extract_tz(os.path.join(p_path, fso))\n",
    "    \n",
    "    # get record and workout data\n",
    "    record = extract_record(os.path.join(p_path, fso))\n",
    "    workout = extract_workout(os.path.join(p_path, fso))\n",
    "    bpm = extract_bpm(os.path.join(p_path, fso))\n",
    "    \n",
    "    # update data to the timezones\n",
    "    record = adjust_timezone(record, tz, ['endDate'])\n",
    "    workout = adjust_timezone(workout, tz, ['startDate', 'endDate'])\n",
    "    bpm = adjust_timezone(bpm, tz, ['time'])    \n",
    "    \n",
    "    # rename bpm columns\n",
    "    bpm = bpm.rename(columns = {'time':'timestamp', 'bpm': 'hr'})\n",
    "    \n",
    "    # extract the metrics from the record data\n",
    "    hr = extract_apple_metric(record, 'HKQuantityTypeIdentifierHeartRate', 'hr', lambda x: x.mean(), [bpm])\n",
    "    dist = extract_apple_metric(record, 'HKQuantityTypeIdentifierDistanceWalkingRunning', 'dist', lambda x: x.sum()*1000)\n",
    "    steps = extract_apple_metric(record, 'HKQuantityTypeIdentifierStepCount', 'steps', lambda x: x.sum())\n",
    "    cals = extract_apple_metric(record, 'HKQuantityTypeIdentifierActiveEnergyBurned', 'cals', lambda x: x.sum())\n",
    "    \n",
    "    # extract activities data\n",
    "    act = extract_apple_activities(workout)\n",
    "    \n",
    "    # combine metrics into a single dataframe and adds the device name\n",
    "    data = pd.concat([hr, dist, steps, cals, act], axis=1)\n",
    "    data = add_device(p_path, data, 'Smartwatch')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca28b3",
   "metadata": {},
   "source": [
    "## Process raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(path, known_fsos, dir_name):\n",
    "    '''\n",
    "    Prints any unknown file system objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to the directory\n",
    "    known_fsos: list of strings\n",
    "        list of known file system objects in the directory\n",
    "    dir_name: string\n",
    "        name of the directory being searched\n",
    "    '''\n",
    "    unknowns = []\n",
    "    for fso in os.listdir(path):\n",
    "        if not any(fso.startswith(known_fso) for known_fso in known_fsos):\n",
    "            unknowns.append(fso)\n",
    "\n",
    "    if len(unknowns) > 0:\n",
    "        print('Unknown file system objects in {} directory: {}'.format(dir_name, unknowns))\n",
    "        \n",
    "\n",
    "def process_raw_data(raw_path, save_path):\n",
    "    '''\n",
    "    Processes the raw BrisT1D data and saves it in a consitant format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_path: string\n",
    "        path to the raw data\n",
    "    save_path: string\n",
    "        path to where the processed data will be saved\n",
    "    '''\n",
    "    data_extracts = {\n",
    "        'apple_health': extract_apple,\n",
    "        'carelink_pump_': extract_carelink_pump,\n",
    "        'carelink_sensor_': extract_carelink_sensor,\n",
    "        'clarity_': extract_clarity,\n",
    "        'fitbit': extract_dashboard,\n",
    "        'glooko': extract_glooko,\n",
    "        'libreview.csv': extract_libreview,\n",
    "        'takeout': extract_takeout\n",
    "    }\n",
    "    \n",
    "    other_fsos = [\n",
    "        'basal_profile.txt',\n",
    "        'devices.csv',\n",
    "        'device_carelink.txt',\n",
    "        'extended.csv'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    medtronic_loops = ['P12', 'P16', 'P18']\n",
    "    omnipod_5_starts = {'P07': pd.to_datetime('2023-07-05 10:00'), 'P17': pd.to_datetime('2023-06-01 00:00')}\n",
    "\n",
    "    for p_num in os.listdir(raw_path):\n",
    "        print('Participant {}'.format(p_num[1:]))\n",
    "        p_path = os.path.join(raw_path, p_num)\n",
    "        check_directory(p_path, list(data_extracts.keys())+other_fsos, p_num)\n",
    "        p_data = pd.DataFrame()\n",
    "\n",
    "        # consider glooko files together for pump and phone misalingment detection\n",
    "        for data_extract in data_extracts:\n",
    "            fsos = [fso for fso in os.listdir(p_path) if fso.startswith(data_extract)]\n",
    "            if fsos:\n",
    "                print(fsos)\n",
    "                if data_extract == 'glooko':\n",
    "                    if p_num in omnipod_5_starts:\n",
    "                        p_data = pd.concat([p_data, data_extracts['glooko'](p_path, fsos, omnipod_5_starts[p_num])])\n",
    "                    else:\n",
    "                        p_data = pd.concat([p_data, data_extracts['glooko'](p_path, fsos)])\n",
    "                else:\n",
    "                    for fso in fsos:\n",
    "                        # remove data after the change to carelink exporting that removed closed-loop insulin adjustments for those that use one\n",
    "                        if p_num in medtronic_loops and fso.startswith('carelink_pump_') and dt.datetime.strptime(fso[-14:-4], '%Y-%m-%d') > dt.datetime.strptime('2023-12-01', '%Y-%m-%d'):\n",
    "                            print('carelink file \"{}\" ignored due to missing closed-loop adjustments from later carelink exports'.format(fso))\n",
    "                        else:\n",
    "                            p_data = pd.concat([p_data, data_extracts[data_extract](p_path, fso)])\n",
    "\n",
    "        # resorts the data, removes duplicates, and removes data after the last insulin value\n",
    "        p_data = p_data.sort_index().reset_index().drop_duplicates().reset_index()\n",
    "        p_data = p_data.set_index('timestamp')\n",
    "        all_columns = ['bg', 'insulin', 'carbs', 'hr', 'dist', 'steps', 'cals', 'activity', 'device']\n",
    "        columns = []\n",
    "        for column in all_columns:\n",
    "            if column not in p_data.columns:\n",
    "                p_data[column] = np.nan\n",
    "        p_data = p_data[all_columns]\n",
    "\n",
    "        # round columms\n",
    "        p_data['insulin'] = p_data['insulin'].round(4)\n",
    "        p_data['hr'] = p_data['hr'].round(1)\n",
    "        p_data['dist'] = p_data['dist'].round(1)\n",
    "        p_data['steps'] = p_data['steps'].round(0)\n",
    "        p_data['cals'] = p_data['cals'].round(2)\n",
    "\n",
    "        # prints all the data and plots the t1d data\n",
    "        display(p_data)\n",
    "        plot_data(p_data)\n",
    "        print()\n",
    "\n",
    "        # saves the data as a .csv file\n",
    "        p_data.to_csv(os.path.join(save_path, p_num + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938aa204",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = '../data/raw_state'\n",
    "save_path = '../data/processed_state/'\n",
    "\n",
    "process_raw_data(raw_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f480d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brist1d_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
